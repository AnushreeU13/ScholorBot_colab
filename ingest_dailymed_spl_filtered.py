"""
Ingest DailyMed SPL bulk releases into a FAISS vector store (MedCPT version).

Key upgrades in this overwrite:
1) Bigger per-label section coverage: MAX_BLOCKS_PER_LABEL increased (prevents "indications-only" coverage).
2) Section priority: warnings/adverse/interactions/dosage/contra/populations FIRST, indications LAST.
3) Source date in citations: loads DailyMed UPLOAD_DATE + TITLE from spl_setid_zipmeta.json
   generated by build_spl_setid_whitelist.py.
4) Evidence-friendly metadata: stores source_title/source_date/spl_version so citations can show date.

IMPORTANT:
- Uses MedCPT Article Encoder for KB chunk embeddings.
- If you change embedding model, you MUST rebuild the FAISS index.
"""

from __future__ import annotations

import io
import re
import json
import zipfile
import xml.etree.ElementTree as ET
from pathlib import Path
from typing import Dict, List, Tuple, Iterable

import numpy as np
import torch

from config import PROJECT_ROOT, FAISS_INDICES_DIR
from embedding_utils import MedCPTEmbedder
from storage_utils import create_faiss_store


# -----------------------------
# Config: paths
# -----------------------------
PROJECT_ROOT = Path(PROJECT_ROOT)
SPL_ZIP_DIR = PROJECT_ROOT / "datasets" / "KB_raw" / "druglabels" / "spl_full_release_zips"
OUT_PROCESSED = PROJECT_ROOT / "datasets" / "KB_processed" / "druglabels_text"
OUT_PROCESSED.mkdir(parents=True, exist_ok=True)

STORE_NAME = "kb_druglabels_medcpt"

WHITELIST_PATH = OUT_PROCESSED / "spl_target_inner_zip_paths.txt"
ZIPMETA_JSON = OUT_PROCESSED / "spl_setid_zipmeta.json"
OUT_CHUNKS_JSONL = OUT_PROCESSED / "druglabels_chunks.jsonl"


# -----------------------------
# FAST + ACCURACY settings
# -----------------------------
MAX_CHARS_PER_LABEL = 60000

# Increased coverage to avoid only capturing "INDICATIONS" sections.
MAX_BLOCKS_PER_LABEL = 6

MAX_CHUNKS_PER_BLOCK = 3
CHUNK_SIZE = 900
OVERLAP = 100
PRINT_EVERY_XML = 25

# Embedding settings (doc/chunk side)
EMBED_BATCH_SIZE = 4
DOC_MAX_LEN = 384


def section_group_from_title(section_title: str) -> str:
    """Map section heading to a normalized section group for routing/reranking."""
    t = (section_title or "").lower()

    if "boxed warning" in t:
        return "warnings"
    if "warnings" in t or "precautions" in t:
        return "warnings"
    if "dosage" in t or "administration" in t:
        return "dosage"
    if "adverse" in t:
        return "adverse"
    if "interactions" in t:
        return "interactions"
    if "contraindication" in t:
        return "contraindications"
    if "specific populations" in t or "use in specific populations" in t:
        return "populations"
    if "indications" in t or "usage" in t:
        return "indications"
    return "other"


def _strip_text(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def _xml_to_text_fast(xml_bytes: bytes) -> str:
    """
    Parse SPL XML and concatenate all text nodes.
    itertext() is usually faster than iterating nodes + .text.
    """
    try:
        root = ET.fromstring(xml_bytes)
    except Exception:
        return ""
    text = " ".join(t.strip() for t in root.itertext() if t and t.strip())
    return _strip_text(text)


def _fixed_chunks(text: str, chunk_size: int, overlap: int, max_chunks: int) -> List[str]:
    """Fast deterministic chunking."""
    chunks: List[str] = []
    start = 0
    while start < len(text) and len(chunks) < max_chunks:
        end = min(len(text), start + chunk_size)
        chunks.append(text[start:end])
        if end == len(text):
            break
        start = end - overlap
    return chunks


def _load_innerpath_to_zipmeta(json_path: Path) -> Dict[str, Dict]:
    """
    Load inner_zip_path -> meta lookup.
    Expected JSON rows from build_spl_setid_whitelist.py:
      {
        "inner_zip_path": "prescription/<ZIP_FILE_NAME>",
        "upload_date": "...",
        "spl_version": "...",
        "title": "..."
      }
    """
    if not json_path.exists():
        return {}

    rows = json.loads(json_path.read_text(encoding="utf-8"))
    m: Dict[str, Dict] = {}
    for r in rows:
        inner_path = (r.get("inner_zip_path") or "").strip()
        if inner_path:
            m[inner_path] = r
    return m


def _extract_key_blocks_fast(full_text: str) -> List[Tuple[str, str]]:
    """
    Lightweight section-aware extraction using regex headings.

    Priority order matters. We want high clinical value sections FIRST:
    1) Boxed warning / warnings
    2) Adverse reactions
    3) Drug interactions
    4) Dosage and administration
    5) Contraindications / populations
    6) Indications (last)

    This is fast + reproducible; not perfect structure parsing, but high-value.
    """
    if not full_text:
        return []

    text = full_text[:MAX_CHARS_PER_LABEL]

    # IMPORTANT: priority order
    heads = [
        "BOXED WARNING",
        "WARNINGS AND PRECAUTIONS",
        "WARNINGS",
        "ADVERSE REACTIONS",
        "DRUG INTERACTIONS",
        "DOSAGE AND ADMINISTRATION",
        "DOSAGE",
        "CONTRAINDICATIONS",
        "USE IN SPECIFIC POPULATIONS",
        "INDICATIONS AND USAGE",
        "INDICATIONS",
    ]

    pattern = r"(?P<h>\b(" + "|".join([re.escape(h) for h in heads]) + r")\b)"
    matches = list(re.finditer(pattern, text, flags=re.IGNORECASE))

    blocks: List[Tuple[str, str]] = []

    if matches:
        # Extract candidate blocks
        for i, m in enumerate(matches):
            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)
            title = text[m.start():m.end()].strip()
            body = _strip_text(text[m.end():end].strip())
            if len(body) >= 300:
                blocks.append((_strip_text(title), body))
    else:
        blocks = [("FULL_LABEL_TRUNC", _strip_text(text))]

    # Re-rank blocks by priority (so we keep the most useful sections)
    pri = {
        "warnings": 1,
        "adverse": 2,
        "interactions": 3,
        "dosage": 4,
        "contraindications": 5,
        "populations": 6,
        "indications": 7,
        "other": 99,
    }

    blocks_sorted = sorted(
        blocks,
        key=lambda tb: pri.get(section_group_from_title(tb[0]), 99),
    )

    return blocks_sorted[:MAX_BLOCKS_PER_LABEL]


def _iter_xml_from_whitelist(zf: zipfile.ZipFile, inner_zip_paths: Iterable[str]):
    """
    Yield (virtual_path, xml_bytes, container_type) for whitelisted inner zips
    that exist inside the given outer zip file.
    """
    outer_names = set(zf.namelist())

    for inner_path in inner_zip_paths:
        if inner_path not in outer_names:
            continue

        try:
            inner_bytes = zf.read(inner_path)
            with zipfile.ZipFile(io.BytesIO(inner_bytes), "r") as inner:
                xml_name = None
                for n in inner.namelist():
                    if n.lower().endswith(".xml"):
                        xml_name = n
                        break
                if not xml_name:
                    continue

                xml_bytes = inner.read(xml_name)
                virtual_path = f"{inner_path}::{xml_name}"
                yield (virtual_path, xml_bytes, "inner_zip_xml")

        except Exception:
            continue


def main():
    if not SPL_ZIP_DIR.exists():
        raise FileNotFoundError(f"Missing SPL zip directory: {SPL_ZIP_DIR}")

    zip_paths = sorted(SPL_ZIP_DIR.glob("dm_spl_release_human_rx_part*.zip"))
    if not zip_paths:
        raise FileNotFoundError(f"No SPL part zips found under: {SPL_ZIP_DIR}")

    print(f"[INFO] SPL zip directory: {SPL_ZIP_DIR}")
    print(f"[INFO] Found {len(zip_paths)} SPL zip parts")

    # -----------------------------
    # Load whitelist
    # -----------------------------
    if not WHITELIST_PATH.exists():
        raise FileNotFoundError(
            f"Whitelist file not found: {WHITELIST_PATH}\n"
            "Run: python build_spl_setid_whitelist.py first."
        )

    target_inner_zip_paths = set(
        p.strip() for p in WHITELIST_PATH.read_text(encoding="utf-8").splitlines()
        if p.strip()
    )
    print(f"[INFO] Whitelist inner zips: {len(target_inner_zip_paths)}")

    # -----------------------------
    # Load DailyMed meta lookup (for citation date/title)
    # -----------------------------
    innerpath2meta = _load_innerpath_to_zipmeta(ZIPMETA_JSON)
    print(f"[INFO] Loaded zip meta rows: {len(innerpath2meta)} (for source_date/title)")

    # -----------------------------
    # Initialize embedding + store
    # -----------------------------
    embedder = MedCPTEmbedder()  # Article encoder for document chunks
    dim = embedder.dim
    print(f"[OK] MedCPT embedding dimension: {dim}")

    store = create_faiss_store(STORE_NAME, dimension=dim, base_dir=FAISS_INDICES_DIR)

    torch.set_grad_enabled(False)

    all_chunk_texts: List[str] = []
    all_metadata: List[Dict] = []

    scanned_xml = 0
    matched_xml = 0
    emitted_chunks = 0

    # -----------------------------
    # Scan outer zip parts (whitelist only)
    # -----------------------------
    for zp in zip_paths:
        print(f"\n[INFO] Scanning zip: {zp.name}")
        with zipfile.ZipFile(zp, "r") as zf:
            names = zf.namelist()
            print(f"  - total entries: {len(names)}")

            outer_names = set(names)
            in_this_part_list = [p for p in target_inner_zip_paths if p in outer_names]
            print(f"  - whitelisted inner zips in this part: {len(in_this_part_list)}")

            for virtual_path, xml_bytes, container_type in _iter_xml_from_whitelist(zf, in_this_part_list):
                scanned_xml += 1

                if scanned_xml % PRINT_EVERY_XML == 0:
                    print(f"  - progress scanned_xml={scanned_xml}, matched_xml={matched_xml}, chunks={emitted_chunks}")

                full_text = _xml_to_text_fast(xml_bytes)
                if not full_text or len(full_text) < 800:
                    continue

                blocks = _extract_key_blocks_fast(full_text)
                if not blocks:
                    continue

                matched_xml += 1

                # Link to DailyMed meta using the inner zip path (before ::xml)
                inner_zip_path = virtual_path.split("::", 1)[0]
                zm = innerpath2meta.get(inner_zip_path, {})

                for section_title, section_text in blocks:
                    chunks = _fixed_chunks(
                        section_text,
                        chunk_size=CHUNK_SIZE,
                        overlap=OVERLAP,
                        max_chunks=MAX_CHUNKS_PER_BLOCK,
                    )

                    for text in chunks:
                        if len(text) < 200:
                            continue

                        group = section_group_from_title(section_title)

                        all_chunk_texts.append(text)
                        all_metadata.append({
                            "source": "DailyMed",
                            "doc_type": "druglabel_spl",
                            "zip_part": zp.name,
                            "container_type": container_type,
                            "virtual_xml_path": virtual_path,
                            "source_id": virtual_path,  # stable locator
                            "matched_keywords": ["whitelist"],
                            "section_title": section_title,
                            "section_group": group,
                            "audience": "clinician",
                            "preset": "fast_section",
                            "embed_model": "MedCPT-Article-Encoder",

                            # Citation-critical fields (source date is NOT retrieved_at)
                            "source_title": zm.get("title"),
                            "source_date": zm.get("upload_date"),
                            "spl_version": zm.get("spl_version"),
                        })
                        emitted_chunks += 1

    print("\n[INFO] Scan finished")
    print(f"  - scanned_xml: {scanned_xml}")
    print(f"  - matched_xml: {matched_xml}")
    print(f"  - total_chunks: {len(all_chunk_texts)}")

    if not all_chunk_texts:
        print("[WARN] No chunks produced. Check whitelist contents and zip structure.")
        return

    # -----------------------------
    # CHECKPOINT: save chunks BEFORE embedding
    # -----------------------------
    with open(OUT_CHUNKS_JSONL, "w", encoding="utf-8") as f:
        for t, m in zip(all_chunk_texts, all_metadata):
            rec = dict(m)
            rec["text"] = t
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")
    print(f"[CHECKPOINT] Wrote chunks to: {OUT_CHUNKS_JSONL}")

    # -----------------------------
    # Embed + FAISS (memory-safe)
    # -----------------------------
    print("[INFO] Embedding chunks (batched)...")
    embs = embedder.embed_texts(
        all_chunk_texts,
        batch_size=EMBED_BATCH_SIZE,
        max_length=DOC_MAX_LEN,
    ).astype(np.float32)

    print("[INFO] Adding to FAISS store...")
    store.add_vectors(embs, all_metadata)
    store.save()

    stats = store.get_stats()
    print("\n[DONE] Saved kb_druglabels_medcpt index + metadata")
    print(stats)


if __name__ == "__main__":
    main()
